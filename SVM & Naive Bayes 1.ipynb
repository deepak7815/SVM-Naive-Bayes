{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89dfc56-2150-4c9a-a387-943c3f28603d",
   "metadata": {},
   "source": [
    "\n",
    "SVM & Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef329c7-5e41-4247-9e48-6a1c94a00ae1",
   "metadata": {},
   "source": [
    "1 : What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b7e03-2a35-46a1-b36e-a9b0ae4fb5ad",
   "metadata": {},
   "source": [
    "What is Information Gain?\n",
    "\n",
    "Information Gain (IG) measures how much uncertainty (entropy) is reduced after splitting a dataset based on a feature.\n",
    "\n",
    " In simple words:\n",
    "\n",
    "Information Gain tells us which feature is the best to split the data at each node of a Decision Tree.\n",
    "The feature with the highest Information Gain is chosen for the split.\n",
    "\n",
    " Why is Information Gain Needed?\n",
    "\n",
    "When building a decision tree:\n",
    "\n",
    "We want to create pure nodes (where data mostly belongs to one class).\n",
    "Information Gain helps select the most informative feature that best separates the classes.\n",
    "\n",
    " Key Concepts\n",
    "1️ Entropy\n",
    "\n",
    "Entropy measures the impurity or randomness in a dataset.\n",
    "\n",
    "How Information Gain is Used in Decision Trees:-\n",
    "\n",
    "Start with the full dataset.\n",
    "Calculate entropy of the target variable.\n",
    "\n",
    "For each feature:\n",
    "\n",
    "Split the data.\n",
    "Compute entropy after the split.\n",
    "Calculate Information Gain.\n",
    "Choose the feature with maximum Information Gain.\n",
    "\n",
    "Repeat recursively until:\n",
    "\n",
    "Nodes are pure, or\n",
    "Stopping criteria are met (depth, minimum samples).\n",
    "\n",
    " Limitations of Information Gain\n",
    "\n",
    "Biased toward features with many unique values (e.g., ID column).\n",
    "Can lead to overfitting.\n",
    "\n",
    " That’s why algorithms like C4.5 use Gain Ratio instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d920c8-0268-4a13-95ab-2bc7421a58bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a22ee8-8245-446e-8136-6483ff76e6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3df273-d161-4a12-8272-1c952c538a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "578bffc3-87aa-44c8-8996-b1c604c8bdeb",
   "metadata": {},
   "source": [
    "2: What is the difference between Gini Impurity and Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f818822-1b3d-4167-a7dc-171920c69eb6",
   "metadata": {},
   "source": [
    "ini Impurity vs Entropy\n",
    "\n",
    "Both Gini Impurity and Entropy are impurity measures used by decision tree algorithms to decide the best split at each node.\n",
    "\n",
    " Definitions:-\n",
    "Gini Impurity\n",
    "\n",
    "Measures the probability of incorrect classification of a randomly chosen element.\n",
    "Used by CART (Classification and Regression Trees).\n",
    "\n",
    "Entropy\n",
    "\n",
    "Measures the amount of uncertainty or randomness in the data.\n",
    "Used by ID3 and C4.5 algorithms.\n",
    "\n",
    "Gini Impurity\n",
    "\n",
    "Strengths\n",
    "\n",
    "Faster to compute\n",
    "Works well for large datasets\n",
    "Default choice in many libraries (e.g., sklearn)\n",
    "\n",
    "Weaknesses\n",
    "\n",
    "Slightly less informative for complex distributions\n",
    "Less sensitive to class imbalance\n",
    "\n",
    "Entropy\n",
    "\n",
    "Strengths\n",
    "\n",
    "More theoretically sound (information theory)\n",
    "More sensitive to class purity\n",
    "Often produces more informative splits\n",
    "\n",
    "Weaknesses\n",
    "\n",
    "Computationally expensive\n",
    "Can lead to deeper trees (overfitting risk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f907713-0e6f-45a4-8ab0-40edb735736b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab4cd3-cd71-411c-b2ae-e7aaf32f143f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e5f0c-d03b-4807-94d3-8c12271c6ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8921bdf5-021b-452f-9fb1-3052d8abbed7",
   "metadata": {},
   "source": [
    "3:What is Pre-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6717e-bc11-4120-ad48-2952ef32ea39",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Pre-Pruning (also called Early Stopping) is a technique where the growth of a decision tree is stopped early during training to prevent the tree from becoming too complex.\n",
    "\n",
    " In simple words:\n",
    "\n",
    "The tree is not allowed to grow fully; splitting stops when certain conditions are met.\n",
    "\n",
    " Why Pre-Pruning is Needed\n",
    "\n",
    "Decision Trees can:\n",
    "\n",
    "Grow very deep\n",
    "Learn noise from training data\n",
    "Perform poorly on unseen data (overfitting)\n",
    "\n",
    "Pre-Pruning helps:\n",
    "✔ Reduce overfitting\n",
    "✔ Improve generalization\n",
    "✔ Reduce training time\n",
    "✔ Create simpler, interpretable trees\n",
    "\n",
    " How Pre-Pruning Works\n",
    "\n",
    "Before making a split at any node, the algorithm checks predefined stopping criteria.\n",
    "If the criteria are not satisfied, the node becomes a leaf.\n",
    "\n",
    " Common Pre-Pruning Criteria\n",
    "\n",
    "Maximum Tree Depth\n",
    " Stop splitting if the tree reaches a fixed depth.\n",
    "Minimum Samples per Split\n",
    " Require a minimum number of samples to split a node.\n",
    "Minimum Samples per Leaf\n",
    " Ensure each leaf has enough data points.\n",
    "Minimum Impurity Decrease\n",
    " Split only if impurity reduction (Gini/Entropy) is above a threshold.\n",
    "Maximum Number of Leaf Nodes\n",
    " Limit the total number of leaf nodes.\n",
    "\n",
    "Limitations of Pre-Pruning\n",
    "\n",
    "May stop too early\n",
    "Risk of underfitting\n",
    "Choosing optimal thresholds can be tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f47f2-bc13-4e52-8f84-fa8c15b969d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07903572-9829-4f0a-95bf-cb4d58b94a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bf0f7-5e75-4134-aa7e-c835b7542a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "748f13e7-d11a-4e73-890c-a51251476d62",
   "metadata": {},
   "source": [
    "4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a38927-7dbc-47bc-bd02-537e1bf9b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances (Gini Impurity):\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0167\n",
      "petal length (cm): 0.9061\n",
      "petal width (cm): 0.0772\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "feature_names = iris.feature_names\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# 2. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train Decision Tree using Gini Impurity\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Print feature importances\n",
    "print(\"Feature Importances (Gini Impurity):\")\n",
    "for feature, importance in zip(feature_names, model.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e3641-b88f-47e4-bdbd-6d38e940210e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fb5b3-d324-47fa-aa3f-6143bdf570a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c130786c-190f-410c-9dc6-7b79e3b7a0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db51a61f-b3bf-41b5-972d-72efa78ede75",
   "metadata": {},
   "source": [
    "5: What is a Support Vector Machine (SVM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d6253-f820-4e29-836b-ad01d80b0fad",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
    "Its main goal is to find the optimal decision boundary (hyperplane) that best separates different classes in the data.\n",
    "\n",
    " Key Idea Behind SVM\n",
    "\n",
    " SVM tries to:\n",
    "\n",
    "Separate data points into classes\n",
    "Maximize the margin between the closest data points of each class\n",
    "These closest points are called Support Vectors.\n",
    "\n",
    " Core Concepts\n",
    "1 Hyperplane\n",
    "\n",
    "A line (2D), plane (3D), or higher-dimensional boundary\n",
    "Separates data points of different classes\n",
    "\n",
    "2️ Margin\n",
    "\n",
    "Distance between the hyperplane and the nearest data points\n",
    "SVM chooses the hyperplane with the maximum margin\n",
    "\n",
    "3️ Support Vectors\n",
    "\n",
    "Data points closest to the hyperplane\n",
    "They define the position of the hyperplane\n",
    "\n",
    " Types of SVM::-\n",
    " Linear SVM:\n",
    "\n",
    "Used when data is linearly separable\n",
    "Decision boundary is a straight line\n",
    "\n",
    " Non-Linear SVM:\n",
    "\n",
    "Used when data is not linearly separable\n",
    "Uses kernel functions to map data into higher dimensions\n",
    "\n",
    "Advantages of SVM\n",
    "\n",
    "- Effective in high-dimensional spaces\n",
    "- Works well with small and medium datasets\n",
    "- Robust to overfitting (with proper kernel & C value)\n",
    "\n",
    " Disadvantages of SVM\n",
    "\n",
    "- Computationally expensive for large datasets\n",
    "- Sensitive to choice of kernel and parameters\n",
    "- Harder to interpret than Decision Trees\n",
    "\n",
    "Example::-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7ed9c2-a564-4e7c-9bf0-ac8bf03f01a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model trained successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset\n",
    "data = datasets.load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"SVM model trained successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4f12b-c74d-4d78-8460-673d1a41a3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c2ed2-6e17-49d7-827c-b1a97956938b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1a24a-116c-4de2-944c-fa44e54f9495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1bb86e4-8824-4b79-91dc-09645e9cfa4c",
   "metadata": {},
   "source": [
    "6: What is the Kernel Trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdd2ef-790f-44a4-a282-26c5cac042f4",
   "metadata": {},
   "source": [
    "The Kernel Trick is a technique used in Support Vector Machines (SVM) that allows the algorithm to separate non-linearly separable data by implicitly mapping it into a higher-dimensional space, without explicitly computing the transformation.\n",
    "\n",
    " Why the Kernel Trick is Needed ?\n",
    "\n",
    "Some datasets cannot be separated by a straight line in their original feature space.\n",
    "\n",
    " Example:\n",
    "\n",
    "XOR problem\n",
    "Circular or spiral data patterns\n",
    "Instead of manually transforming features, SVM uses a kernel function to compute inner products in a higher-dimensional space efficiently.\n",
    "\n",
    " Key Idea (Simple Explanation)\n",
    "\n",
    "The kernel trick lets SVM draw non-linear decision boundaries in the original space by performing linear separation in a higher-dimensional space.\n",
    "\n",
    " How It Works (Conceptually)\n",
    "\n",
    "Data is mapped from input space → higher-dimensional feature space\n",
    "A linear hyperplane is found in that space\n",
    "When projected back, the boundary appears non-linear\n",
    "\n",
    "Advantages of Kernel Trick:\n",
    "\n",
    " Handles non-linear data efficiently\n",
    " No need to explicitly compute higher-dimensional features\n",
    " Powerful and flexible decision boundaries\n",
    "\n",
    " Limitations:\n",
    "\n",
    " Kernel choice is problem-dependent\n",
    " Can be computationally expensive for large datasets\n",
    " Harder to interpret\n",
    "\n",
    "Example::-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "908c152e-b785-497a-a479-65d551f85408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM trained using Kernel Trick (RBF kernel)\n"
     ]
    }
   ],
   "source": [
    "#Example::-\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create non-linear data\n",
    "X, y = make_circles(n_samples=200, noise=0.1, factor=0.2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svm_model = SVC(kernel='rbf', gamma='scale')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"SVM trained using Kernel Trick (RBF kernel)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871fcf1-1ae8-421f-a924-b6fbbc3cd2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d482b-9ab9-4694-a533-e036b05e95dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166f351-9506-40ca-9d3e-a50e46fe5925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56441585-4fbe-4c0c-8113-9ad81f1bcd9e",
   "metadata": {},
   "source": [
    "7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6ef084-6805-4676-a142-ed42450fea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Linear Kernel SVM: 1.0\n",
      "Accuracy with RBF Kernel SVM: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# 2. Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train SVM with Linear kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# 4. Train SVM with RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# 6. Calculate accuracies\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# 7. Print results\n",
    "print(\"Accuracy with Linear Kernel SVM:\", accuracy_linear)\n",
    "print(\"Accuracy with RBF Kernel SVM:\", accuracy_rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766ca75-0db8-45bf-97d2-cef9a3d53ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c89051-13ac-4a46-ac82-a1249f8fb4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290f51c-644a-4994-b807-46e99ec66d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f40d144-94a6-425b-8afd-e9a0a9419a1c",
   "metadata": {},
   "source": [
    "8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c08dff-1e61-4460-a872-6ea7e0064fc8",
   "metadata": {},
   "source": [
    "What is Naïve Bayes?\n",
    "\n",
    "The Naïve Bayes classifier is a supervised machine learning algorithm based on Bayes’ Theorem.\n",
    "It is mainly used for classification tasks, especially in text classification, spam detection, and sentiment analysis.\n",
    "\n",
    "It predicts the class of a data point by calculating the posterior probability for each class and choosing the class with the highest probability.\n",
    "\n",
    "Why is it Called “Naïve”?\n",
    "\n",
    "It is called “Naïve” because of its strong assumption:\n",
    "\n",
    " All features are conditionally independent given the class label\n",
    "\n",
    "This assumption is usually not true in real-world data, but surprisingly, the algorithm still works very well in practice.\n",
    "\n",
    "Key Assumptions:\n",
    "\n",
    "Features are independent of each other\n",
    "Each feature contributes equally and independently to the outcome\n",
    "No correlation among predictors\n",
    "\n",
    "Advantages\n",
    "\n",
    " Simple and fast\n",
    " Works well with high-dimensional data\n",
    " Performs well even with small datasets\n",
    " Very effective for text classification\n",
    "\n",
    " Disadvantages\n",
    "\n",
    " Strong independence assumption\n",
    " Poor performance if features are highly correlated\n",
    " Zero probability problem (handled using smoothing)\n",
    "\n",
    " Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7873fd-f792-46f9-aa88-93a12ba4608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes model trained successfully\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Naïve Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Naïve Bayes model trained successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3e053-06ca-41ec-9447-7fe74412e43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7fabcd-00a4-4282-98db-0365adf2ef61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0786749-b155-4340-bda3-244e5d24cd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57720020-4f5e-4e09-a08d-58fb52b16f3a",
   "metadata": {},
   "source": [
    "10: Breast Cancer Dataset\n",
    "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
    "dataset and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deaca1bc-940a-4666-b078-80782d6bb6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Naïve Bayes Classifier: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train Gaussian Naïve Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# 5. Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy of Gaussian Naïve Bayes Classifier:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83aae6-c986-4f25-8946-4025e9b625bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
